---
title: "modelStudio - R & Python examples"
author: "Hubert Baniecki"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{modelStudio - Python & R examples}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(
  collapse = FALSE,
  comment = "#>",
  warning = FALSE,
  message = FALSE,
  eval = FALSE
)
```

# R & Python Examples

The `modelStudio()` function uses `DALEX` explainers created with `DALEX::explain()` or `DALEXtra::explain_*()`.   

```{r eval = FALSE}
# update main dependencies
install.packages("ingredients")
install.packages("iBreakDown")

# packages for explainer objects
install.packages("DALEX")
install.packages("DALEXtra")
```

## R

### mlr [dashboard](https://modeloriented.github.io/modelStudio/mlr.html)

In this example we will fit a `ranger` model on `titanic` data.

```{r eval = FALSE}
# load packages and data
library(mlr)
library(DALEXtra)
library(modelStudio)

data <- DALEX::titanic_imputed

# split the data
index <- sample(1:nrow(data), 0.8*nrow(data))
train <- data[index, ]
test <- data[-index, ]

# mlr ClassifTask takes target as factor
train$survived <- as.factor(train$survived)

# fit a model
task <- makeClassifTask(id = "titanic",
                        data = train,
                        target = "survived")

learner <- makeLearner("classif.ranger",
                       predict.type = "prob")

model <- train(learner, task)

# create an explainer for the model
explainer <- explain_mlr(model,
                         data = test,
                         y = test$survived,
                         label = "mlr")

# pick observations
new_observation <- test[1:2, ]
rownames(new_observation) <- c("id1", "id2")

# make a studio for the model
modelStudio(explainer,
            new_observation)
```

### mlr3 [dashboard](https://modeloriented.github.io/modelStudio/mlr3.html)

In this example we will fit a `ranger` model on `titanic` data.

```{r eval = FALSE}
# load packages and data
library(mlr3)
library(mlr3learners)
library(DALEXtra)
library(modelStudio)

data <- DALEX::titanic_imputed

# split the data
index <- sample(1:nrow(data), 0.8*nrow(data))
train <- data[index, ]
test <- data[-index, ]

# mlr3 TaskClassif takes target as factor
train$survived <- as.factor(train$survived)

# fit a model
task <- TaskClassif$new(id = "titanic",
                        backend = train,
                        target = "survived")

learner <- lrn("classif.ranger",
               predict_type = "prob")

learner$train(task)

# create an explainer for the model
explainer <- explain_mlr3(learner,
                          data = test,
                          y = test$survived,
                          label = "mlr3")

# pick observations
new_observation <- test[1:2, ]
rownames(new_observation) <- c("id1", "id2")

# make a studio for the model
modelStudio(explainer,
            new_observation)
```

### xgboost [dashboard](https://modeloriented.github.io/modelStudio/xgboost.html)

In this example we will fit a `xgboost` model on `titanic` data.

```{r eval = FALSE}
# load packages and data
library(xgboost)
library(DALEX)
library(modelStudio)

data <- DALEX::titanic_imputed

# split the data
index <- sample(1:nrow(data), 0.8*nrow(data))
train <- data[index, ]
test <- data[-index, ]

train_matrix <- model.matrix(survived ~.-1, train)
test_matrix <- model.matrix(survived ~.-1, test)

# fit a model
xgb_matrix <- xgb.DMatrix(train_matrix, label = train$survived)
params <- list(eta = 0.01, subsample = 0.6, max_depth = 7, min_child_weight = 3,
               objective = "binary:logistic", eval_metric = "auc")
model <- xgb.train(params, xgb_matrix, nrounds = 1000)

# create an explainer for the model
explainer <- explain(model,
                     data = test_matrix,
                     y = test$survived,
                     label = "xgboost")

# pick observations
new_observation <- test_matrix[1:2,,drop=FALSE]
rownames(new_observation) <- c("id1", "id2")

# make a studio for the model
modelStudio(explainer,
            new_observation,
            options = modelStudioOptions(margin_left = 140))
```

### caret [dashboard](https://modeloriented.github.io/modelStudio/caret.html)

In this example we will fit a `gbm` model on `titanic` data.

```{r eval = FALSE}
# load packages and data
library(caret)
library(DALEX)
library(modelStudio)

data <- DALEX::titanic_imputed

# split the data
index <- sample(1:nrow(data), 0.8*nrow(data))
train <- data[index, ]
test <- data[-index, ]

# caret train takes target as factor
train$survived <- as.factor(train$survived)

# fit a model
cv <- trainControl(method = "repeatedcv",
                   number = 3,
                   repeats = 10)

model <- train(survived ~ ., data = train,
               method = "gbm",
               trControl = cv,
               verbose = FALSE)

# create an explainer for the model
explainer <- explain(model,
                     data = test,
                     y = test$survived,
                     label = "caret")

# pick observations
new_observation <- test[1:2,]
rownames(new_observation) <- c("id1", "id2")

# make a studio for the model
modelStudio(explainer,
            new_observation)
```

### h2o [dashboard](https://modeloriented.github.io/modelStudio/h2o.html)

In this example we will fit a `h2o.automl` model on `titanic` data.

```{r eval = FALSE}
# load packages and data
library(h2o)
library(DALEXtra)
library(modelStudio)

data <- DALEX::titanic_imputed

# init h2o
h2o::h2o.init()

# split the data
h2o_split <- h2o.splitFrame(as.h2o(data))
train <- h2o_split[[1]]
test <- as.data.frame(h2o_split[[2]])

# h2o automl takes target as factor
train$survived <- as.factor(train$survived)

# fit a model
automl <- h2o.automl(y = "survived",
                     training_frame = train,
                     max_runtime_secs = 30)
model <- automl@leader

# stop h2o progress printing
h2o.no_progress()

# create an explainer for the model
explainer <- explain_h2o(model,
                         data = test,
                         y = test$survived,
                         label = "h2o")

# pick observations
new_observation <- test[1:2, ]
rownames(new_observation) <- c("id1", "id2")

# make a studio for the model
modelStudio(explainer,
            new_observation,
            B = 5)

# shutdown h2o
h2o::h2o.shutdown(prompt = FALSE)
```


## Python

The `modelStudio()` function uses `dalex` explainers created with `dalex.Explainer()`.

```{bash, eval=FALSE, engine="sh"}
pip3 install dalex --force
```

Use `pickle` Python module and `reticulate` R package to easily make a studio for a model.

### scikit-learn [dashboard](https://modeloriented.github.io/modelStudio/scikitlearn.html)

In this example we will fit a `Pipeline MLPClassifier` model on `titanic` data.

First, use `dalex` in Python:

```{r eval = FALSE}
# load packages and data
import dalex as dx

from sklearn.model_selection import train_test_split
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.impute import SimpleImputer
from sklearn.compose import ColumnTransformer
from sklearn.neural_network import MLPClassifier

data = dx.datasets.load_titanic()
X = data.drop(columns='survived')
y = data.survived

# split the data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=1)

# fit a pipeline model
numeric_features = ['age', 'fare', 'sibsp', 'parch']
numeric_transformer = Pipeline(
  steps=[
    ('imputer', SimpleImputer(strategy='median')),
    ('scaler', StandardScaler())
    ]
)
categorical_features = ['gender', 'class', 'embarked']
categorical_transformer = Pipeline(
  steps=[
    ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),
    ('onehot', OneHotEncoder(handle_unknown='ignore'))
    ]
)

preprocessor = ColumnTransformer(
  transformers=[
    ('num', numeric_transformer, numeric_features),
    ('cat', categorical_transformer, categorical_features)
    ]
)

model = Pipeline(
  steps=[
    ('preprocessor', preprocessor),
    ('classifier', MLPClassifier(hidden_layer_sizes=(150,100,50), max_iter=500, random_state=0))
    ]
)
model.fit(X_train, y_train)

# create an explainer for the model
explainer = dx.Explainer(model, X_test, y_test, label = 'scikit-learn')

#! remove residual_function before dump !
explainer.residual_function = None

# pack the explainer into a pickle file
import pickle
pickle_out = open("explainer_scikitlearn.pickle","wb")
pickle.dump(explainer, pickle_out)
pickle_out.close()
```

Then, use `modelStudio` in R:

```{r eval = FALSE}
# load the explainer from the pickle file
library(reticulate)
explainer <- py_load_object('explainer_scikitlearn.pickle', pickle = "pickle")

# make a studio for the model
library(modelStudio)
modelStudio(explainer)
```

### lightGBM [dashboard](https://modeloriented.github.io/modelStudio/lightgbm.html)

In this example we will fit a `Pipeline LGBMClassifier` model on `titanic` data.

First, use `dalex` in Python:

```{r eval = FALSE}
# load packages and data
import dalex as dx

from sklearn.model_selection import train_test_split
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.impute import SimpleImputer
from sklearn.compose import ColumnTransformer
from lightgbm import LGBMClassifier

data = dx.datasets.load_titanic()
X = data.drop(columns='survived')
y = data.survived

# split the data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=1)

# fit a pipeline model
numeric_features = ['age', 'fare', 'sibsp', 'parch']
numeric_transformer = Pipeline(
  steps=[
    ('imputer', SimpleImputer(strategy='median')),
    ('scaler', StandardScaler())
    ]
)
categorical_features = ['gender', 'class', 'embarked']
categorical_transformer = Pipeline(
  steps=[
    ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),
    ('onehot', OneHotEncoder(handle_unknown='ignore'))
    ]
)

preprocessor = ColumnTransformer(
  transformers=[
    ('num', numeric_transformer, numeric_features),
    ('cat', categorical_transformer, categorical_features)
    ]
)

model = Pipeline(
  steps=[
    ('preprocessor', preprocessor),
    ('classifier', LGBMClassifier())
    ]
)
model.fit(X_train, y_train)

# create an explainer for the model
explainer = dx.Explainer(model, X_test, y_test, label = 'lightGBM')

#! remove residual_function before dump !
explainer.residual_function = None

# pack the explainer into a pickle file
import pickle
pickle_out = open("explainer_lightgbm.pickle","wb")
pickle.dump(explainer, pickle_out)
pickle_out.close()  
```

Then, use `modelStudio` in R:

```{r eval = FALSE}
# load the explainer from the pickle file
library(reticulate)
explainer <- py_load_object('explainer_lightgbm.pickle', pickle = "pickle")

# make a studio for the model
library(modelStudio)
modelStudio(explainer)
```

### keras/tensorflow [dashboard](https://modeloriented.github.io/modelStudio/keras.html)

In this example we will fit a `Pipeline KerasClassifier` model on `titanic` data.

First, use `dalex` in Python:

```{r eval = FALSE}
# load packages and data
import dalex as dx

from sklearn.model_selection import train_test_split
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.impute import SimpleImputer
from sklearn.compose import ColumnTransformer
from keras.wrappers.scikit_learn import KerasClassifier
from keras.layers import Dense
from keras.models import Sequential

data = dx.datasets.load_titanic()
X = data.drop(columns='survived')
y = data.survived

# split the data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=1)

# fit a pipeline model
numeric_features = ['age', 'fare', 'sibsp', 'parch']
numeric_transformer = Pipeline(
  steps=[
    ('imputer', SimpleImputer(strategy='median')),
    ('scaler', StandardScaler())
    ]
)
categorical_features = ['gender', 'class', 'embarked']
categorical_transformer = Pipeline(
  steps=[
    ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),
    ('onehot', OneHotEncoder(handle_unknown='ignore'))
    ]
)

preprocessor = ColumnTransformer(
  transformers=[
    ('num', numeric_transformer, numeric_features),
    ('cat', categorical_transformer, categorical_features)
    ]
)

def create_architecture():
  model = Sequential()
  # after the pipeline there are 17 inputs
  model.add(Dense(60, input_dim=17, activation='relu'))
  model.add(Dense(1, activation='sigmoid'))
  model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])
  return model

kc = KerasClassifier(build_fn=create_architecture, epochs=100, batch_size=5, verbose=0)

model = Pipeline(
  steps=[
    ('preprocessor', preprocessor),
    ('classifier', kc)
    ]
)
model.fit(X_train, y_train)

# create an explainer for the model
explainer = dx.Explainer(model, X_test, y_test, label = 'keras')

#! remove residual_function before dump !
explainer.residual_function = None

# pack the explainer into a pickle file
import pickle
pickle_out = open("explainer_keras.pickle","wb")
pickle.dump(explainer, pickle_out)
pickle_out.close() 
```

Then, use `modelStudio` in R:

```{r eval = FALSE}
# load the explainer from the pickle file
library(reticulate)

#! add blank create_architecture function before load !
py_run_string("
def create_architecture():
    return True
"
)
explainer <- py_load_object('explainer_keras.pickle', pickle = "pickle")

# make a studio for the model
library(modelStudio)
modelStudio(explainer)
```

## References

* Theoretical introduction to the plots: [Explanatory Model Analysis. Explore, Explain and Examine Predictive Models.](https://pbiecek.github.io/ema)
* Wrapper for the function is implemented in [DALEX](https://modeloriented.github.io/DALEX/)
* Feature Importance, Ceteris Paribus, Partial Dependence and Accumulated Dependence plots 
are implemented in [ingredients](https://modeloriented.github.io/ingredients/)
* Break Down and Shapley Values plots are implemented in [iBreakDown](https://modeloriented.github.io/iBreakDown/)
